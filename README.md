# Attention-Transfer-for-Knowledge-Distillation

PyTorch code for "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer

https://cloud.githubusercontent.com/assets/4953728/22037632/04f54a7e-dd09-11e6-9a6b-62133fbc1c29.png






Activation-based spatial attention transfer implementation
Knowledge distillation implementation
Similarity-preserving knowledge distillation implementation
